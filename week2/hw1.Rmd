---
title: "[ESC 21FALL] Homework 1"
author: "Your Name :D"
output:
  pdf_document: default
  html_document: default
---

### Part 1. Implement and experience KDE on your own!  
```{r}
X = sort(runif(200, min=0, max=4*pi)) # generate random number btw 0~4*pi
Y = sin(X) + rnorm(200, sd=0.3)       # add noise to sin function
plot(X, Y, pch=20)                    # draw scatterplot
```
  
First, let's see how `ksmooth` function works in default R.  
```{r}
Kreg = ksmooth(x=X, y=Y, kernel="normal", bandwidth=0.5)
plot(X, Y, pch=20)
lines(Kreg, lwd=4, col="purple")
```
  
(a) Check how it is different from above when you use box kernel with same bandwith.
```{r}
# TODO
```
  
(b) Implement your own kernel function from scratch!  
```{r}
ksmooth.train <- function(x.train, y.train, bandwidth = 0.5) {
  # kernel should be scaled so that their quartiles
  # (viewed as probability densities) are at +/- 0.25*bandwidth
  sigma = 0.25*bandwidth/qnorm(0.75, 0, 1)
  # define Gaussian kernel
  kern <- function(x) # TODO
  
  # empty list to store yhat (f hat) values
  yhat.train = numeric(length(x.train))
  for (i in 1:length(x.train)) {
      yhat.train[i] = # TODO
  }
  ksmooth.train.out = cbind(x.train, yhat.train)
  
  return(ksmooth.train.out)
}
```
  
(c) Check if you did well :)  
```{r}
Kreg = ksmooth(x=X, y=Y, kernel="normal", bandwidth=0.5)
myKreg = ksmooth.train(x.train=X, y.train=Y, bandwidth=0.5)
plot(X, Y, pch=20)
lines(Kreg, lwd=2, col="purple")
lines(myKreg, lty=3, lwd=4, col="green2")
```
  
(d) Let's do it on more realistic dataset.
```{r}
source('home1-part1-data.R')
```
  
Produce a scatterplot of `wage.train` vs `age.train` and add a kernel smooth for a `normal` kernel with `bandwidth = 3`. Observe the residual sum of squares.
```{r}
smooth = # TODO
age.train = # TODO
wage.train = # TODO
RSS.train = # TODO
cat("RSS.train : ", RSS.train)
plot(Wage.train$age, Wage.train$wage)
lines(wage.train, col = 'blue2', lwd=3)
```
  
Plot the training error of the expected squared prediction error as a function of bandwidth for bandwidths = 1, 2, ..., 10. Print the 10 values and explain the result briefly.
```{r}
training_error = numeric(10)
for (i in 1:10) {
  trained = # TODO
  training_error[i] = # TODO
}
print(training_error)
plot(1:10, training_error, type = 'b')
```
(Now, we will continue to experiment bias-variance tradeoff in optimal bandwidth problem)  
  
### Part 2. Optimal Bandwidth (refer to hw description in `README.md`)
(a) Using a Gaussian kernel $\phi_{\sigma}$, plot squared bias, variance, and their sum for $\sigma=$`seq(from = 0.01, to = 2, by = 0.01)`. Print the optimal choice for $\sigma$.
```{r}
source('home1-part2-data.R')
```
(This process takes some time...!)  
```{r}
# Initialization 
sigma = seq(from = 0.01, to = 2, by = 0.01)
n = length(sigma)
squared_norm <- function(x) sum(x^2)  # will be used for computing bias^2

# initialize empty list to store values
bias = numeric(n)          # stores bias^2
variance = numeric(n)
summation = numeric(n)

for (k in 1:n) {
  # W : weight matrix (kernel function value)
  # make sure to include normalizing part!
  W = matrix(nrow = n, ncol = n) 
  for (i in 1:n) {         # move filter (kernel) through query point
    for (j in 1:n) {       # local neighborhood (all data due to Gauessian kernel)
      W[i,j] = # TODO
    }
  }
  variance[k] = noise.var * sum(diag(t(W)%*%W))
  bias[k] = squared_norm(# TODO)
  summation[k] = variance[k] + bias[k] # MSE
}

# Plotting
plot(sigma, bias, type = 'l', col = 'red', xlab = 'sigma', ylab='')
lines(sigma, variance, col = 'blue')
lines(sigma, summation, col = 'green')
legend('left', legend = c("Bias", "Variance", "Sum"), 
       col = c("red", "blue", "green"), pch=16)
```
  
(b) Plot the training sample, $f$, and $\hat{f}$ for the optimal choice of $\sigma$.
```{r}
opt = # TODO
W = matrix(nrow = n, ncol = n)
for (i in 1:n) {
  for (j in 1:n) {
    W[i,j] = # TODO
  }
}
plot(x.train, y.train)
lines(x.train, f, col = 'red')
f.hat = # TODO
lines(x.train, f.hat, col = 'blue')
legend('topright', legend = c("f", "f.hat"), col = c("red", "blue"), lty = 1)
```
  
(c) Check the output for simulated data in Part 1.
```{r}
Kreg1 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 0.1)
Kreg2 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 0.9)
Kreg3 = ksmooth(x=X,y=Y,kernel = "normal",bandwidth = 3.0)
plot(X,Y,pch=20)
lines(Kreg1, lwd=3, col="orange")
lines(Kreg2, lwd=3, col="purple")
lines(Kreg3, lwd=3, col="limegreen")
legend("topright", c("h=0.1","h=0.9","h=3.0"), lwd=6,
col=c("orange","purple","limegreen"))
```